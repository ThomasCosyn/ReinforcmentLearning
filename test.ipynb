{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym.spaces import Box\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'\n",
    "\n",
    "min_replay_size = 2\n",
    "buffer_size = 4\n",
    "replay_buffer = deque(maxlen = buffer_size)\n",
    "device = 'cpu'\n",
    "learning_rate = 0.1\n",
    "batch_size = 2\n",
    "discount_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema = Constants.schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate, discrete_action_space = np.linspace(-1, 1, num = 21)):\n",
    "\n",
    "        super(DQN,self).__init__()\n",
    "        input_features = env.observation_space[0].shape[0]\n",
    "        self.discrete_action_space = discrete_action_space\n",
    "\n",
    "        self.dense1 = nn.Linear(in_features = input_features, out_features = 128)\n",
    "        self.dense2 = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.dense3 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.dense4 = nn.Linear(in_features = 32, out_features = len(discrete_action_space))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "\n",
    "    def __init__(self, env, buffer_size, min_replay_size = 1000):\n",
    "\n",
    "        self.env = env\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.reward_buffer = deque([-200.0], maxlen = 100)\n",
    "\n",
    "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        discrete_action_space = np.linspace(-1, 1, num = 21)\n",
    "        for _ in range(self.min_replay_size):\n",
    "\n",
    "            action = [[np.random.choice(discrete_action_space)] for i in range(len(env.action_space))]\n",
    "            new_obs, rew, done, _ = env.step(action)\n",
    "            transition = (obs, action, rew, done, new_obs)\n",
    "            self.replay_buffer.append(transition)\n",
    "            obs = new_obs\n",
    "\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "\n",
    "        print('Initialization with random transitions is done!')\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.replay_buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # Echantillonage d'un batch de transitions\n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "        observations = np.asarray([t[0] for t in transitions])\n",
    "        actions = np.asarray([t[1] for t in transitions])\n",
    "        rewards = np.asarray([t[2] for t in transitions])\n",
    "        dones = np.asarray([t[3] for t in transitions])\n",
    "        new_observations = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        observations_t = torch.as_tensor(observations, dtype = torch.float32)\n",
    "        actions_t = torch.as_tensor(actions, dtype = torch.float32).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype = torch.float32).unsqueeze(-1)\n",
    "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32)\n",
    "\n",
    "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        self.reward_buffer.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the experience replay buffer will be filled with random transitions\n",
      "Initialization with random transitions is done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (dense1): Linear(in_features=28, out_features=128, bias=True)\n",
       "  (dense2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (dense3): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (dense4): Linear(in_features=32, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_memory = ExperienceReplay(env, buffer_size)\n",
    "online_network = DQN(env, learning_rate).to(device)\n",
    "online_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.40000000000000013]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "discrete_action_space = np.linspace(-1, 1, num = 21)\n",
    "action = [np.random.choice(discrete_action_space)]\n",
    "observation = env.observations[2]\n",
    "obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "q_values = online_network(obs_t.unsqueeze(0))\n",
    "max_q_index = torch.argmax(q_values, dim = 1)\n",
    "action = [discrete_action_space[max_q_index.item()]]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0396, grad_fn=<SmoothL1LossBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample random transitions with size = batch size\n",
    "observations_t, actions_t, rewards_t, dones_t, new_observations_t = replay_memory.sample(batch_size)\n",
    "building = np.random.randint(0, len(env.observation_space))\n",
    "# Compute the target value\n",
    "target_q_values = online_network(new_observations_t[:,building,:])\n",
    "max_target_q_values = target_q_values.max(dim = 1, keepdim = True)[0]\n",
    "targets = rewards_t[:,building] + discount_rate * (1 - dones_t) * max_target_q_values\n",
    "# Compute the loss\n",
    "q_values = online_network(observations_t[:,building,:])\n",
    "a = actions_t[:,building,0]\n",
    "a_index = torch.as_tensor([j for i in range(len(a)) for j in range(len(discrete_action_space)) if abs(a[i] - discrete_action_space[j]) < 0.001])\n",
    "a_index = a_index.reshape((2,1))\n",
    "action_q_values = torch.gather(q_values, dim = 1, index = a_index)\n",
    "loss = F.smooth_l1_loss(action_q_values, targets)\n",
    "# Gradient descent\n",
    "online_network.optimizer.zero_grad()\n",
    "loss.backward()\n",
    "online_network.optimizer.step()\n",
    "# print(\"q_values : {}\".format(q_values.shape))\n",
    "# print(\"actions : {}\".format(actions_t[:,building,0].shape))\n",
    "# print(\"targets : {}\".format(targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0182, -0.3201,  0.0802,  0.0085, -0.2231,  0.1789, -0.0635, -0.0429,\n",
      "         -0.1486,  0.0853, -0.2203, -0.0698,  0.1285, -0.1239,  0.3671,  0.2435,\n",
      "          0.1823, -0.0274,  0.2194, -0.3401,  0.1225],\n",
      "        [ 0.0386, -0.3158,  0.0568,  0.0190, -0.2125,  0.1685, -0.0807, -0.0635,\n",
      "         -0.1477,  0.0872, -0.2017, -0.0986,  0.1551, -0.1360,  0.3629,  0.2425,\n",
      "          0.1816, -0.0236,  0.2160, -0.3413,  0.1440]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "torch.Size([2, 1])\n",
      "tensor([[16],\n",
      "        [16]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1823],\n",
       "        [0.1816]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = actions_t[:,building,0]\n",
    "das = np.linspace(-1, 1, num = 21)\n",
    "print(q_values)\n",
    "a_index = torch.as_tensor([j for i in range(len(a)) for j in range(len(das)) if abs(a[i] - das[j]) < 0.001])\n",
    "a_index = a_index.reshape((2,1))\n",
    "print(a_index.shape)\n",
    "print(a_index)\n",
    "action_q_values = torch.gather(q_values, dim = 1, index = a_index)\n",
    "action_q_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b16107990d103b1efe2ab53987c5f14ba77eda8b60cd08ceec849048e8954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
