{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\t.cosyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym.spaces import Box\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des classes permettant d'instancier l'environnement CityLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabrication du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate):\n",
    "\n",
    "        super(DQN,self).__init__()\n",
    "        input_features = env.observation_space[0].shape[0]\n",
    "        action_space = 21\n",
    "\n",
    "        self.dense1 = nn.Linear(in_features = input_features, out_features = 128)\n",
    "        self.dense2 = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.dense3 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.dense4 = nn.Linear(in_features = 32, out_features = action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "\n",
    "    def __init__(self, env, buffer_size, min_replay_size = 1000):\n",
    "\n",
    "        self.env = env\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.reward_buffer = deque([-200.0], maxlen = 100)\n",
    "\n",
    "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        discrete_action_space = np.linspace(-1, 1, num = 21)\n",
    "        for _ in range(self.min_replay_size):\n",
    "\n",
    "            action = [[np.random.choice(discrete_action_space)] for i in range(len(env.action_space))]\n",
    "            new_obs, rew, done, _ = env.step(action)\n",
    "            transition = (obs, action, rew, done, new_obs)\n",
    "            self.replay_buffer.append(transition)\n",
    "            obs = new_obs\n",
    "\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "\n",
    "        print('Initialization with random transitions is done!')\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.replay_buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # Echantillonage d'un batch de transitions\n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "        observations = np.asarray([t[0] for t in transitions])\n",
    "        actions = np.asarray([t[1] for t in transitions])\n",
    "        rewards = np.asarray([t[2] for t in transitions])\n",
    "        dones = np.asarray([t[3] for t in transitions])\n",
    "        new_observations = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        observations_t = torch.as_tensor(observations, dtype = torch.float32)\n",
    "        actions_t = torch.as_tensor(actions, dtype = torch.float32).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype = torch.float32).unsqueeze(-1)\n",
    "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32)\n",
    "\n",
    "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        self.reward_buffer.append(reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqAgent:\n",
    "\n",
    "    def __init__(self, env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size):\n",
    "        \n",
    "        # Définition des attributs\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # Instanciation de l'ExperienceReplay et du réseau de neurones\n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n",
    "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "\n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "\n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "\n",
    "        random_sample = random.random()\n",
    "\n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            action = [self.env.action_space[0].sample() for _ in range(len(self.env.action_space))]\n",
    "        \n",
    "        else:\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "\n",
    "            max_q_index = torch.argmax(q_values, dim = 2)\n",
    "            action = maction = [[max_q_index.detach()[0][i].item()] for i in range(len(self.env.action_space))]\n",
    "\n",
    "        return action, epsilon\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "\n",
    "        # On échantillone batch_size transitions random\n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "        target_q_values = self.online_network(new_observations_t)\n",
    "        max_target_q_values = target_q_values.max(dim = 2, keepdim = True)[0]\n",
    "        targets = rewards_t + self.discount_rate * (1 - dones_t) * max_target_q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the experience replay buffer will be filled with random transitions\n",
      "Initialization with random transitions is done!\n"
     ]
    }
   ],
   "source": [
    "env = CityLearnEnv(schema = Constants.schema_path)\n",
    "obs = env.reset()\n",
    "replay_buffer = deque(maxlen = 100)\n",
    "\n",
    "replay_memory = ExperienceReplay(env, 100)\n",
    "\n",
    "online_network = DQN(env, 0.1).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m online_network(new_observations_t)\n\u001b[0;32m      4\u001b[0m max_target_q_values \u001b[38;5;241m=\u001b[39m target_q_values\u001b[38;5;241m.\u001b[39mmax(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m targets \u001b[38;5;241m=\u001b[39m rewards_t \u001b[38;5;241m+\u001b[39m \u001b[43mdiscount_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdones_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_target_q_values\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "discount_rate = 0.99\n",
    "observations_t, actions_t, rewards_t, dones_t, new_observations_t = replay_memory.sample(100)\n",
    "target_q_values = online_network(new_observations_t)\n",
    "max_target_q_values = target_q_values.max(dim = 2, keepdim = True)[0]\n",
    "targets = rewards_t + discount_rate * (1 - dones_t) * max_target_q_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b16107990d103b1efe2ab53987c5f14ba77eda8b60cd08ceec849048e8954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
