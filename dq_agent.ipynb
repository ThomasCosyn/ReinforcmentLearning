{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gym.spaces import Box\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des classes permettant d'instancier l'environnement CityLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    episodes = 3\n",
    "    schema_path = './data/citylearn_challenge_2022_phase_1/schema.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabrication du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate, discrete_action_space = np.linspace(-1, 1, num = 21)):\n",
    "\n",
    "        super(DQN,self).__init__()\n",
    "        input_features = env.observation_space[0].shape[0]\n",
    "        self.discrete_action_space = discrete_action_space\n",
    "\n",
    "        self.dense1 = nn.Linear(in_features = input_features, out_features = 128)\n",
    "        self.dense2 = nn.Linear(in_features = 128, out_features = 64)\n",
    "        self.dense3 = nn.Linear(in_features = 64, out_features = 32)\n",
    "        self.dense4 = nn.Linear(in_features = 32, out_features = len(discrete_action_space))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = torch.tanh(self.dense2(x))\n",
    "        x = torch.tanh(self.dense3(x))\n",
    "        x = torch.tanh(self.dense4(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "\n",
    "    def __init__(self, env, buffer_size, min_replay_size = 1000):\n",
    "\n",
    "        self.env = env\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.replay_buffer = deque(maxlen = buffer_size)\n",
    "        self.reward_buffer = deque([-200.0], maxlen = 100)\n",
    "\n",
    "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        discrete_action_space = np.linspace(-1, 1, num = 21)\n",
    "        for _ in range(self.min_replay_size):\n",
    "\n",
    "            action = [[np.random.choice(discrete_action_space)] for i in range(len(env.action_space))]\n",
    "            new_obs, rew, done, _ = env.step(action)\n",
    "            transition = (obs, action, rew, done, new_obs)\n",
    "            self.replay_buffer.append(transition)\n",
    "            obs = new_obs\n",
    "\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "\n",
    "        print('Initialization with random transitions is done!')\n",
    "\n",
    "    def add_data(self, data):\n",
    "        self.replay_buffer.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # Echantillonage d'un batch de transitions\n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "        observations = np.asarray([t[0] for t in transitions])\n",
    "        actions = np.asarray([t[1] for t in transitions])\n",
    "        rewards = np.asarray([t[2] for t in transitions])\n",
    "        dones = np.asarray([t[3] for t in transitions])\n",
    "        new_observations = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "        # Conversion en tensors\n",
    "        observations_t = torch.as_tensor(observations, dtype = torch.float32)\n",
    "        actions_t = torch.as_tensor(actions, dtype = torch.float32).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype = torch.float32).unsqueeze(-1)\n",
    "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32)\n",
    "\n",
    "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t\n",
    "\n",
    "    def add_reward(self, reward):\n",
    "        self.reward_buffer.append(reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqAgent:\n",
    "\n",
    "    def __init__(self, env, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size, discrete_action_space):\n",
    "        \n",
    "        # Définition des attributs\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        self.discrete_action_space = discrete_action_space\n",
    "\n",
    "        # Instanciation de l'ExperienceReplay et du réseau de neurones\n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size)\n",
    "        self.online_network = DQN(self.env, self.learning_rate, self.discrete_action_space).to(self.device)\n",
    "\n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "\n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "\n",
    "        random_sample = random.random()\n",
    "\n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            action = [self.env.action_space[0].sample() for _ in range(len(self.env.action_space))]\n",
    "        \n",
    "        else:\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "\n",
    "            max_q_index = torch.argmax(q_values, dim = 1)\n",
    "            action = [self.discrete_action_space[max_q_index.item()]]\n",
    "\n",
    "        return action, epsilon\n",
    "\n",
    "    def learn(self, batch_size):\n",
    "\n",
    "        # Sample random transitions with size = batch size\n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "        building = np.random.randint(0, len(self.env.observation_space))\n",
    "        # Compute the target value\n",
    "        target_q_values = self.online_network(new_observations_t[:,building,:])\n",
    "        max_target_q_values = target_q_values.max(dim = 1, keepdim = True)[0]\n",
    "        targets = rewards_t[:,building] + self.discount_rate * (1 - dones_t) * max_target_q_values\n",
    "        # Compute the loss\n",
    "        q_values = self.online_network(observations_t[:,building,:])\n",
    "        a = actions_t[:,building,0]\n",
    "        a_index = torch.as_tensor([j for i in range(len(a)) for j in range(len(self.discrete_action_space)) if abs(a[i] - self.discrete_action_space[j]) < 0.001])\n",
    "        a_index = a_index.reshape((2,1))\n",
    "        action_q_values = torch.gather(q_values, dim = 1, index = a_index)\n",
    "        loss = F.smooth_l1_loss(action_q_values, targets)\n",
    "        # Gradient descent\n",
    "        self.online_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.online_network.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "127b16107990d103b1efe2ab53987c5f14ba77eda8b60cd08ceec849048e8954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
